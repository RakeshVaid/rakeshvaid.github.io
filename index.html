<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">

    <meta name="author" content="Rakesh Vaideeswaran" />
    <meta name="keywords" content="Rakesh Vaideeswaran, Amazon, Nova, Foundation Models, AGI, UIUC, Resume, Machine Learning, Deep Learning, Computer Vision, Natural Language Processing, Multimodal Learning" />
    <meta name="robots" content="index" />
    <meta name="revisit-after" content="14 days" />

    <title>Rakesh Vaideeswaran</title>
    <link rel="stylesheet" href="styles.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.1/css/all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">

   <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-H03KWHW1K3"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-H03KWHW1K3');
    </script>

</head>
<!-- <link rel="icon" type="image/png" href="figures/favicon-16x16.png"> -->
<link
rel="icon"
href="data:image/svg+xml,<svg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 100 100'><text y='.9em' font-size='90'>üë®üèª‚Äçüíª</text></svg>"
/>

<body>
    <div class="cover-photo-container">
        <div class="cover-photo"></div>
        <img src="figures/rakvaid.png" alt="Profile Picture" class="profile-picture">
        <div class="profile-details">
            <h2>Rakesh Vaideeswaran</h2>
            <div class="social-links-container">
                <div class="social-links">
                    <a href="mailto:rakeshvaideeswaran.nitt@gmail.com">
                        <i class="fa-regular fa-envelope"></i>
                    </a>
                    <a href="https://scholar.google.com/citations?user=r8i2rq8AAAAJ&hl=en" target="_blank">
                        <i class="ai ai-google-scholar"></i>
                    </a>
                    <a href="https://www.linkedin.com/in/rakeshmahesh/" target="_blank">
                        <i class="fa-brands fa-linkedin-in"></i>
                    </a>
                    <a href="CV.pdf">
                        <i class="fas fa-file-alt"></i>
                    </a>
                    <a href="https://www.worldcubeassociation.org/persons/2012VAID02" target="_blank">
                        <img src="figures/rubik.png" alt="WCA" style="vertical-align:middle;">
                    </a>
                </div>
            </div>
        </div>
    </div>

    <nav>
        <ul>
            <li><a href="#" class="tab active" onclick="showTab('home')">Home</a></li>
            <li><a href="#" class="tab active" onclick="showTab('news')">News</a></li>
            <li><a href="#" class="tab" onclick="showTab('experiences')">Experience</a></li>
            <li><a href="#" class="tab" onclick="showTab('research')">Research</a></li>
            <li><a href="#" class="tab" onclick="showTab('cv')">CV</a></li>
        </ul>
    </nav>

    <main>
        <section id="home" class="tab-content">
            <p>
                Hi, I‚Äôm Rakesh Vaideeswaran, currently working as an Applied Scientist II on the Amazon Artificial General Intelligence team. 
                My work focuses on building large-scale multimodal foundation models that power the next generation of AI applications. 
                I earned my Master‚Äôs degree in Electrical and Computer Engineering from the University of Illinois Urbana-Champaign (UIUC) in December 2022.
            </p>
            
            <p>
                Beyond my work in AI, I like to identify myself as one among the small 
                fraction of people in the world who can solve the 
                3x3 Rubik‚Äôs Cube in under 10 seconds. 
                You can view my official <a href="https://www.worldcubeassociation.org/persons/2012VAID02" target="_blank">Speedcubing Profile here</a>.
            </p>

            <p>
                Over the years, I‚Äôve shared this passion with many kids by teaching them 
                how to solve the cube. Trust me when I say the Rubik‚Äôs Cube will teach you 
                a lot more than just bringing six colours together. For those who are 
                curious to know more, I highly recommend reading this insightful 
                <a href="https://www.cubelelo.com/blogs/cubing/why-should-you-cube" target="_blank">article</a> written by my 
                friend and fellow speedcuber, <a href="https://www.worldcubeassociation.org/persons/2011NARA02">Dr. Bhargav Narasimhan</a>.
            </p>

            <p>
                I strongly relate with the quote: "It always seems impossible until it‚Äôs done."  
                My experience with the Rubik‚Äôs Cube has instilled in me the belief that any 
                challenge‚Äîno matter how difficult‚Äîcan be overcome with persistence, 
                patience, and practice. This continues to motivate me both personally and 
                professionally in every task I pursue.
            </p>
            <p>
                If you are interested in collaborating, would like to discuss research, or have any question feel free to reach out to me 
                at <a href="mailto:rakeshvaideeswaran.nitt@gmail.com">rakeshvaideeswaran.nitt@gmail.com</a>.
            </p>
        </section>

        <section id="news" class="tab-content">
            <ul class="collapsible-list">

                <li class="show"><span class="date">[Jun 2025]</span> Preprint of <a href="https://www.arxiv.org/abs/2506.05543" target="_blank">FRAME</a> is out on arXiv.</li>
                <li class="show"><span class="date">[May 2025]</span> Preprint of <a href="https://arxiv.org/abs/2505.18153" target="_blank">REN</a> is out on arXiv.</li>
                <li class="show"><span class="date">[May 2025]</span> Started as a research intern at Meta.</li>
                <li class="show"><span class="date">[May 2025]</span> <a href="https://arxiv.org/abs/2501.08648" target="_blank">MAGNET</a> got accepted at ACL 2025.</li>
                <li class="show"><span class="date">[Feb 2025]</span> <a href="https://arxiv.org/abs/2412.01826" target="_blank">RELOCATE</a> got accepted at CVPR 2025.</li>
                <li class="show"><span class="date">[Jan 2025]</span> Preprint of <a href="https://arxiv.org/abs/2501.08648" target="_blank">MAGNET</a> is out on arXiv.</li>
                <li class="show"><span class="date">[Jan 2025]</span> Preprint of <a href="https://arxiv.org/abs/2412.01826" target="_blank">RELOCATE</a> is out on arXiv.</li>
                <li class="show"><span class="date">[May 2024]</span> Started as a research intern at Adobe Research.</li>
                <li class="show"><span class="date">[May 2024]</span> Completed my MS in Computer Science from UIUC.</li>
                <li class="show"><span class="date">[Apr 2024]</span> Accepted a CS PhD offer from UIUC.</li>
                <li class="show"><span class="date">[Feb 2024]</span> <a href="https://unified-io-2.allenai.org/" target="_blank">Unified-IO 2</a> got accepted at CVPR 2024.</li>
                <li class="show"><span class="date">[May 2023]</span> Started as a research intern at the Allen Institute for AI.</li>
                <li class="show"><span class="date">[Oct 2022]</span> Began collaborating with the Allen Institute for AI on Unified-IO 2.</li>
                <li class="show"><span class="date">[Aug 2022]</span> Started the thesis-track M.S. in Computer Science at the University of Illinois Urbana-Champaign.</li>
              </ul>              
            <!-- <a href="#" id="read-more">Read More</a> -->
        </section>

        <section id="experiences" class="tab-content" style="display: none;">
            <p>
                Hover over the logos to read more about what I worked on.
            </p>
            <p>
                <b style="font-size: 1.07em;">Research</b><br>
                I have been involved in a range of research projects, collaborating across both industry and academia. My work has focused on a broad array of topics, including multimodal learning, 
                video understanding, natural language processing, active learning, and adversarial learning.
            </p>
            <div class="grid-container">
                <div class="grid-item">
                    <img src="figures/meta.png" alt="Meta Logo">
                    <p><b>Meta</b></p>
                    <p>May 2025 - Aug 2025</p>
                    <div class="description">
                        <li> Working on multimodal representation learning </li>
                    </div>
                </div>
                <div class="grid-item">
                    <img src="figures/adobe.png" alt="Adobe Research Logo">
                    <p><b>Adobe Research</b></p>
                    <p>May 2024 - Aug 2024</p>
                    <div class="description">
                        <li> Developed <a href="https://arxiv.org/abs/2501.08648" target="_blank">MAGNET</a> a method to simultaneously enhance LLMs with generative and representation 
                        learning capabilities </li>
                        <li> The enhanced LLMs can perform open-ended generation, text 
                        infilling, and token-level and sentence-level representation learning </li>
                    </div>
                </div>
                <div class="grid-item">
                    <img src="figures/ai2.png" alt="Allen Institute for AI Logo">
                    <p><b>Allen Institute for AI</b></p>
                    <p>May 2023 - Aug 2023</p>
                    <div class="description">
                        <li> Contributed to <a href="https://unified-io-2.allenai.org/" target="_blank">Unified-IO 2</a>, 
                        an instruction-following model that can parse and generate multimodal data and perform 120+ tasks </li>
                        <li> Worked on a memory-augmented multimodal encoder for understanding videos ranging from a 
                        few seconds to tens of minutes </li>
                    </div>
                </div>
                <div class="grid-item">
                    <img src="figures/nus.png" alt="NUS Logo">
                    <p><b>National University of Singapore</b></p>
                    <p>Apr 2022 - Aug 2022</p>
                    <div class="description">
                        <li> Developed robust <a href="https://arxiv.org/pdf/2211.00928" target="_blank">active learning algorithm for handling heteroskedastic noise</a>, 
                        resulting in 10% accuracy boost over baselines </li>
                        <li> Demonstrated 15% accuracy improvement in other state-of-the-art algorithms by incorporating a simple 
                        self-supervised approach </li>
                    </div>
                </div>
                <div class="grid-item">
                    <img src="figures/mila.png" alt="Mila Logo">
                    <p><b>Mila</b></p>
                    <p>Apr 2021 - Nov 2021</p>
                    <div class="description">
                        <li> Demonstrated <a href="https://openreview.net/forum?id=6Mfsc-BYp2d" target="_blank">catastrophic failure of uncertainty-based active learning algorithms</a> 
                        by proposing 3 heteroskedastic data distributions </li>
                        <li> Proposed <a href="https://www.sciencedirect.com/science/article/pii/S0893608022002684" target="_blank">interpolated adversarial training</a> 
                        that gives 48% reduction in error rate on clean data while preserving adversarial robustness </li>
                    </div>
                </div>
                <div class="grid-item">
                    <img src="figures/dtu.png" alt="DTU Logo">
                    <p><b>Delhi Technological University</b></p>
                    <p>Apr 2021 - Nov 2021</p>
                    <div class="description">
                        <li> Leveraged image-based malware binary representations and techniques like ensembling and autoencoding to develop 
                        <a href="https://link.springer.com/article/10.1007/s11042-022-12615-7" target="_blank">S-DCNN</a> and 
                        <a href="https://ieeexplore.ieee.org/document/9498570" target="_blank">AE-DCNN</a>, CNNs for malware classification </li>
                        <li> Worked on improving object recognition systems in the presence of adversaries like occlusion and blurriness </li>
                    </div>
                </div>
                <div class="grid-item">
                    <img src="figures/google.png" alt="Google Logo">
                    <p><b>Google</b></p>
                    <p>May 2020 - Jul 2020</p>
                    <div class="description">
                        <li> Initiated the development of <a href="https://economictimes.indiatimes.com/tech/technology/google-aims-to-help-researchers-startups-better-understand-indian-languages/articleshow/79773091.cms" target="_blank">MuRIL</a>, 
                        a BERT-based multilingual language model for 17 Indian dialects and their transliterated versions</li>
                        <li> Achieved a 10.42% F1 improvement in sentiment analysis and a 9.87% in named entity recognition for Indian languages </li>
                    </div>
                </div>
            </div>
            <br><br>
            <p>
                <b style="font-size: 1.07em;">Teaching</b><br>
                I have worked as a teaching assistant, where I was responsible for teaching labs, conducting office hours, grading tests, 
                and mentoring group projects.
            </p>
            <div class="grid-container">
                <div class="grid-item">
                    <img src="figures/uiuc.png" alt="UIUC Logo">
                    <p><b>CS 445: Computational Photography</b></p>
                    <p>Fall 2023</p>
                    <div class="description">
                        <li> Contributed to <a href="https://unified-io-2.allenai.org/" target="_blank">Unified-IO 2</a>, 
                        an instruction-following model that can parse and generate multimodal data and perform 120+ tasks </li>
                        <li> Worked on a memory-augmented multimodal encoder for understanding videos ranging from a 
                        few seconds to tens of minutes </li>
                    </div>
                </div>
                <div class="grid-item">
                    <img src="figures/uiuc.png" alt="UIUC Logo">
                    <p><b>CS 225: Data Structures and Algorithms</b></p>
                    <p>Fall 2022 and Spring 2023</p>
                    <div class="description">
                        <li> Developed a method to simultaneously enhance LLMs with generative and representation 
                        learning capabilities </li>
                        <li> The enhanced LLMs can perform open-ended generation, text 
                        infilling, and token-level and sentence-level representation learning </li>
                    </div>
                </div>
            </div>

            <br><br>
            <p>
                <b style="font-size: 1.07em;">Engineering</b><br>
                I have also worked briefly in software engineering roles (which helped me realize that while I love coding, my 
                true passion lies in research).
            </p>
            <div class="grid-container">
                <div class="grid-item">
                    <img src="figures/google.png" alt="Google Logo">
                    <p><b>Google</b></p>
                    <p>Aug 2021 - Mar 2022</p>
                    <div class="description">
                        <li> Improved Google Search‚Äôs web ranking infrastructure using deep learning for better multimodal 
                        document understanding </li>
                        <li> Enhanced precision and recall in salient entity extraction from webpages by transitioning 
                        from traditional ML methods to LLMs </li>
                    </div>
                </div>
                <div class="grid-item">
                    <img src="figures/cadence.png" alt="Cadence Logo">
                    <p><b>Cadence Design Systems</b></p>
                    <p>Dec 2018 - Jan 2019</p>
                    <div class="description">
                        <li> Developed a unified functionality interface for two version control systems
                        - Perforce and ClearCase </li>
                        <li> Implemented a functionality to streamline complex multi-step process of fetching file revisions 
                        from the two version control systems using a single bash command </li>
                    </div>
                </div>
            </div>
        </section>

        <section id="research" class="tab-content" style="display: none;">
            <p>
                A full list of publications can be seen on my 
                <a href="https://scholar.google.com/citations?user=c0ZJ6IgAAAAJ&hl=en" target="_blank">Google Scholar</a> 
                author page.<br>
                (* denotes equal contribution)
            </p>

            <!-- Publication -->
            <div class="project">
                <img src="figures/frame.png" alt="Project FRAME" class="project-thumbnail">
                <div class="project-details">
                    <p class="pub-title" style="color:#007CD5">
                        
                    </p>
                    <p class="pub-title">
                        <a href="https://www.arxiv.org/abs/2506.05543" target="_blank">FRAME: Pre-Training Video Feature Representations via Anticipation and Memory</a> 
                    </p>
                    <p class="pub-authors">
                        Sethuraman TV, <u>Rakesh Vaideeswaran</u>, Vignesh Srinivasakumar, Jiahui Huang, Seoung Wug Oh, Simon Jenni, Derek Hoiem, Joon-Young Lee
                    </p>
                    <p class="pub-venue">
                        arXiv, 2025
                    </p>
                </div>
            </div>

            <!-- Publication -->
            <div class="project">
                <img src="figures/ren.png" alt="Project REN" class="project-thumbnail">
                <div class="project-details">
                    <p class="pub-title" style="color:#007CD5">
                        
                    </p>
                    <p class="pub-title">
                        <a href="https://arxiv.org/abs/2505.18153" target="_blank">REN: Fast and Efficient Region Encodings from Patch-Based Image Encoders</a> 
                    </p>
                    <p class="pub-authors">
                        <u>Rakesh Vaideeswaran</u>, Sethuraman T V, Barnett Lee, Alexander Schwing, and Derek Hoiem
                    </p>
                    <p class="pub-venue">
                        arXiv, 2025
                    </p>
                </div>
            </div>

            <!-- Publication -->
            <div class="project">
                <img src="figures/relocate.png" alt="Project RELOCATE" class="project-thumbnail">
                <div class="project-details">
                    <p class="pub-title" style="color:#007CD5">
                        
                    </p>
                    <p class="pub-title">
                        <a href="https://arxiv.org/abs/2412.01826" target="_blank">RELOCATE: A Simple Training-Free Baseline for Visual Query Localization Using Region-Based Representations</a> 
                    </p>
                    <p class="pub-authors">
                        <u>Rakesh Vaideeswaran</u>, Sethuraman T V, Alexander Schwing, and Derek Hoiem
                    </p>
                    <p class="pub-venue">
                        Computer Vision and Pattern Recognition, 2025
                    </p>
                </div>
            </div>

            <!-- Publication -->
            <div class="project">
                <img src="figures/magnet.png" alt="Project MAGNET" class="project-thumbnail">
                <div class="project-details">
                    <p class="pub-title" style="color:#007CD5">
                        
                    </p>
                    <p class="pub-title">
                        <a href="https://arxiv.org/abs/2501.08648" target="_blank">MAGNET: Augmenting Generative Decoders with Representation Learning and Infilling Capabilities</a> 
                    </p>
                    <p class="pub-authors">
                        <u>Rakesh Vaideeswaran</u>, Aditi Tiwari, Kushal Kafle, Simon Jenni, Handong Zhao, John Collomosse, and Jing Shi
                    </p>
                    <p class="pub-venue">
                        Association for Computational Linguistics, 2025
                    </p>
                </div>
            </div>

            <!-- Publication -->
            <div class="project">
                <img src="figures/uio.png" alt="Project UIO2" class="project-thumbnail">
                <div class="project-details">
                    <p class="pub-title" style="color:#007CD5">
                        
                    </p>
                    <p class="pub-title">
                        <a href="https://arxiv.org/abs/2312.17172" target="_blank">Unified-IO 2: Scaling Autoregressive Multimodal Model with Vision, Language, Audio, and Action</a> 
                    </p>
                    <p class="pub-authors">
                        Jiasen Lu*, Christopher Clark*, Sangho Lee*, Zichen Zhang*, <u>Rakesh Vaideeswaran</u>, Ryan Marten, Derek Hoiem, and 
                        Aniruddha Kembhavi
                    </p>
                    <p class="pub-venue">
                        Computer Vision and Pattern Recognition, 2024
                    </p>
                </div>
            </div>

            <!-- Publication -->
            <div class="project">
                <img src="figures/memory.png" alt="Project Memory" class="project-thumbnail">
                <div class="project-details">
                    <p class="pub-title">
                        <a href="https://arxiv.org/abs/2312.06141" target="_blank">Survey on Memory-Augmented Neural Networks: Cognitive Insights to AI Applications</a> 
                    </p>
                    <p class="pub-authors">
                        <u>Rakesh Vaideeswaran</u>*, Zhen Zhu*, and Yifie He*
                    </p>
                    <p class="pub-venue">
                        arXiv, 2023
                    </p>
                </div>
            </div>

            <!-- Publication -->
            <div class="project">
                <img src="figures/ecai.png" alt="Project NAL" class="project-thumbnail">
                <div class="project-details">
                    <p class="pub-title">
                        <a href="https://ebooks.iospress.nl/doi/10.3233/FAIA230402" target="_blank">Understanding and Improving Neural Active Learning on Heteroskedastic Distributions</a> 
                    </p>
                    <p class="pub-authors">
                        <u>Rakesh Vaideeswaran</u>, Chew Kin Whye, Jordan T. Ash, Cyril Zhang, Kenji Kawaguchi, 
                        and Alex Lamb
                    </p>
                    <p class="pub-venue">
                        European Conference on Artificial Intelligence, 2023
                    </p>
                </div>
            </div>

            <!-- Publication -->
            <div class="project">
                <img src="figures/neuralnet.png" alt="Project IAT" class="project-thumbnail">
                <div class="project-details">
                    <p class="pub-title">
                        <a href="https://www.sciencedirect.com/science/article/pii/S0893608022002684" target="_blank">Interpolated Adversarial Training: Achieving Robust Neural Networks without Sacrificing too much Accuracy</a> 
                    </p>
                    <p class="pub-authors">
                        Alex Lamb, Vikas Verma, Kenji Kawaguchi, Alexander Matyasko, 
                        <u>Rakesh Vaideeswaran</u>, Juho Kannala, and Yoshua Bengio
                    </p>
                    <p class="pub-venue">
                        Neural Networks, 2022
                    </p>
                </div>
            </div>

            <!-- Publication -->
            <div class="project">
                <img src="figures/sdcnn.png" alt="Project SDCNN" class="project-thumbnail">
                <div class="project-details">
                    <p class="pub-title">
                        <a href="https://link.springer.com/article/10.1007/s11042-022-12615-7" target="_blank">S-DCNN: Stacked Deep Convolutional Neural Networks for Malware Classification</a> 
                    </p>
                    <p class="pub-authors">
                        Anil Singh Parihar, Shashank Kumar, and <u>Rakesh Vaideeswaran</u>
                    </p>
                    <p class="pub-venue">
                        Multimedia Tools and Applications, 2022
                    </p>
                </div>
            </div>

            <!-- Publication -->
            <div class="project">
                <img src="figures/distshift.png" alt="Project AL" class="project-thumbnail">
                <div class="project-details">
                    <p class="pub-title">
                        <a href="https://openreview.net/forum?id=6Mfsc-BYp2d" target="_blank">Catastrophic Failures of Neural Active Learning on Heteroskedastic Distributions</a> 
                    </p>
                    <p class="pub-authors">
                        <u>Rakesh Vaideeswaran</u>, Alex Lamb, Jordan T. Ash, Cyril Zhang, and Kenji Kawaguchi
                    </p>
                    <p class="pub-venue">
                        NeurIPS Workshop on Distribution Shifts, 2021
                    </p>
                </div>
            </div>

            <!-- Publication -->
            <div class="project">
                <img src="figures/muril.png" alt="Project MuRIL" class="project-thumbnail">
                <div class="project-details">
                    <p class="pub-title">
                        <a href="https://arxiv.org/abs/2103.10730" target="_blank">MuRIL: Multilingual Representations for Indian Languages</a> 
                    </p>
                    <p class="pub-authors">
                        Simran Khanuja, Diksha Bansal*, Sarvesh Mehtani*, 
                        <u>Rakesh Vaideeswaran</u>*, Atreyee Dey, Balaji Gopalan, Dilip Kumar Margam, 
                        Pooja Aggarwal, Rajiv Teja Nagipogu, Shachi Dave, Shruti Gupta, 
                        Subhash Chandra Bose Gali, Vish Subramanian, and Partha Talukdar
                    </p>
                    <p class="pub-venue">
                        arXiv, 2021<br>
                        Media Coverage: 
                        <a href="https://economictimes.indiatimes.com/tech/technology/google-aims-to-help-researchers-startups-better-understand-indian-languages/articleshow/79773091.cms" target="_blank">Economic Times</a>, 
                        <a href="https://www.newindianexpress.com/opinions/2020/dec/24/making-use-of-the-language-landscape-diversity-2240471.html" target="_blank">Indian Express</a>, 
                        <a href="https://blog.google/intl/en-in/company-news/outreach-initiatives/l10n-localisation-breaking-down/" target="_blank">Google AI Blog</a>
                    </p>
                </div>
            </div>

            <!-- Publication -->
            <div class="project">
                <img src="figures/aedcnn.png" alt="Project AEDCNN" class="project-thumbnail">
                <div class="project-details">
                    <p class="pub-title">
                        <a href="https://ieeexplore.ieee.org/document/9498570" target="_blank">AE-DCNN: Autoencoder Enhanced Deep Convolutional Neural Network For Malware Classification</a>
                    </p>
                    <p class="pub-authors">
                        Shashank Kumar*, <u>Rakesh Vaideeswaran</u>*, Shivangi Meena, and Anil Singh Parihar
                    </p>
                    <p class="pub-venue">
                        International Conference on Intelligent Technologies, 2021
                    </p>
                </div>
            </div>
        </section>

        <section id="cv" class="tab-content" style="display: none;">
            <div class="resume-container">
                <div class="resume-box">
                <iframe src="CV.pdf" width="80%" height="881px" frameborder="0"></iframe>
                </div>
            </div>
        </section>
    </main>

    <script src="script.js"></script>
</body>

</html>
